{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duanzhihua/-transformer-english2chinese-/blob/main/1_MiniDeepSeek%E5%88%86%E8%AF%8D%E5%99%A8%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3a513ad-0a35-4232-a561-3c258af7729b",
      "metadata": {
        "id": "c3a513ad-0a35-4232-a561-3c258af7729b"
      },
      "source": [
        "# <font face=\"仿宋\">课程说明："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d235a4-eec5-4d67-bdd4-e3147c50c574",
      "metadata": {
        "id": "f3d235a4-eec5-4d67-bdd4-e3147c50c574"
      },
      "source": [
        "&emsp;&emsp;<font face=\"仿宋\">同学们好呀\\~欢迎来到《2025大模型原理与训练》试学体验课！我是课程主讲老师，九天。本期体验课将带领各位同学手动从零到一完成miniDeepSeek大模型训练，完整介绍Ubuntu系统使用、分词器训练、大模型预训练、大模型全量指令微调，以及DPO强化学习微调完整流程！"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b709b8-b2eb-492e-8a7e-ebb0f1a8601c",
      "metadata": {
        "id": "09b709b8-b2eb-492e-8a7e-ebb0f1a8601c"
      },
      "source": [
        "- 体验课内容节选自《2025大模型原理与训练》完整版付费课程"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610f96c3-32fe-463d-9e97-e19b19e00fd3",
      "metadata": {
        "id": "610f96c3-32fe-463d-9e97-e19b19e00fd3"
      },
      "source": [
        "&emsp;&emsp;体验课时间有限，若想深度学习大模型技术，欢迎大家报名由我和菜菜老师主讲的[《2025大模型原理与实战课程》](https://whakv.xetslk.com/s/2I2sk7)："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eb89a64-ca15-4bea-a326-d8bba9d21796",
      "metadata": {
        "id": "4eb89a64-ca15-4bea-a326-d8bba9d21796"
      },
      "source": [
        "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/ae25c01c0cb13278a4b4cf636b940fa.png\" alt=\"ae25c01c0cb13278a4b4cf636b940fa\" style=\"zoom:33%;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7c9b6d-23d4-4d34-8c8f-d6d0f46fadfa",
      "metadata": {
        "id": "dc7c9b6d-23d4-4d34-8c8f-d6d0f46fadfa"
      },
      "source": [
        "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/1736422649885.png\" alt=\"1736422649885\" style=\"zoom:45%;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e3ed408-f9b4-4040-991b-a6d74aed81df",
      "metadata": {
        "id": "2e3ed408-f9b4-4040-991b-a6d74aed81df"
      },
      "source": [
        "**[《2025大模型原理与实战课程》](https://whakv.xetslk.com/s/2I2sk7)上新特惠进行中！限量福利30席售罄即止！课程大纲获取、领取体验课学员专享优惠券，<span style=\"color:red;\">扫码添加课程助教，回复“训练”，即咨询课程信息哦👇</span>**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c543d48-b0ed-4044-af9f-1cb5dc7e5cce",
      "metadata": {
        "id": "9c543d48-b0ed-4044-af9f-1cb5dc7e5cce"
      },
      "source": [
        "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/c6847a817fd7efd0cddcb1bcac217c3.png\" alt=\"c6847a817fd7efd0cddcb1bcac217c3\" style=\"zoom:55%;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff6b8638-85f9-4f22-aa12-68d0e4499a62",
      "metadata": {
        "id": "ff6b8638-85f9-4f22-aa12-68d0e4499a62"
      },
      "source": [
        "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107200452887.png\" alt=\"image-20250107200452887\" style=\"zoom:59%;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "058261aa-a14c-444f-8f42-81d5f187021a",
      "metadata": {
        "id": "058261aa-a14c-444f-8f42-81d5f187021a"
      },
      "source": [
        "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250107200502389.png\" alt=\"image-20250107200502389\" style=\"zoom:50%;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49f1dd5e-c7e5-4638-9a18-78c848b64d41",
      "metadata": {
        "id": "49f1dd5e-c7e5-4638-9a18-78c848b64d41"
      },
      "source": [
        "此外，miniDeepSeek-v3全部训练项目代码、数据、以及训练完的模型，都已上传至课件网盘，扫码联系助教即可领取。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6efdd9ef-e767-4cda-9c68-169c7663979e",
      "metadata": {
        "id": "6efdd9ef-e767-4cda-9c68-169c7663979e"
      },
      "source": [
        "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250109200311025.png\" alt=\"image-20250109200311025\" style=\"zoom:33%;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf3a46f3-f3a3-42bf-902a-3baa92ade2df",
      "metadata": {
        "id": "cf3a46f3-f3a3-42bf-902a-3baa92ade2df"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c6a24c-278b-4ce1-8e5f-bf1900754c75",
      "metadata": {
        "id": "25c6a24c-278b-4ce1-8e5f-bf1900754c75"
      },
      "source": [
        "## <center> 《2025大模型原理与训练》体验课    \n",
        "## <center> 从零手动复现DeepSeek v3\n",
        "## <center> Ch.1 MiniDeepSeek v3 分词器训练过程"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c13f438e-f3ee-4543-be8a-22a12d13e7af",
      "metadata": {
        "id": "c13f438e-f3ee-4543-be8a-22a12d13e7af"
      },
      "source": [
        "### 一、分词器数据集准备\n",
        "\n",
        "​\t在大模型的训练过程中，分词器（Tokenizer）的训练和使用是非常关键的环节之一，直接影响到模型对文本的理解和生成能力。为了更好地理解为何需要训练分词器，以及分词器的概念和作用，下面我将详细解释这些内容。\n",
        "\n",
        "#### 1. 分词器的概念\n",
        "\n",
        "​\t分词器（Tokenizer）是将自然语言文本转换为模型可理解形式的工具，它将连续的文本字符串分解成模型能够处理的基本单位。这些基本单位可以是单词、子词（subword），甚至是单个字符。分词的目的是将原始文本映射为一系列的索引或标记（token），然后输入到模型中进行处理。\n",
        "\n",
        "​\t分词器的工作不仅限于切割文本，还包括将这些分割后的单位映射到模型词汇表中的索引。分词的粒度选择会影响模型的性能、训练时间和生成的结果，因此分词器在训练过程中扮演着重要角色。\n",
        "\n",
        "#### 2. 为什么需要训练分词器？\n",
        "\n",
        "​\t在大模型训练过程中，分词器是模型理解和生成文本的基础。如果没有合理的分词，模型无法有效地学习语言的规律和结构。以下是训练分词器的主要原因：\n",
        "\n",
        "##### 2.1 **处理文本的必要性**\n",
        "\n",
        "​\t计算机无法直接处理自然语言文本，需要将其转换为数字表示。而分词器就是完成这一步的工具，它将文本转换为一系列离散的标记，之后可以通过嵌入层将这些标记转换为向量输入模型。因此，训练分词器的首要任务是高效且准确地将文本分解成适合模型学习的标记。\n",
        "\n",
        "##### 2.2 **减少模型的复杂性**\n",
        "\n",
        "​\t如果直接使用单词作为标记，模型词汇表将变得极其庞大，因为自然语言中有海量的单词及其变体。词汇表过大不仅会导致训练时间增加，还会让模型难以泛化，尤其是当遇到词汇表中未包含的新词时，模型将无法处理。\n",
        "\n",
        "​\t通过训练分词器，尤其是使用子词分词（例如BPE或WordPiece算法），可以将词汇表的大小控制在一个合理的范围内，同时确保模型能够有效处理新词和变体。子词分词的思想是，将常见的词作为整体保留，但对不常见的词进行进一步分解，这样即使是未见过的词，也可以通过子词组合来表示。\n",
        "\n",
        "##### 2.2 **提升模型的泛化能力**\n",
        "\n",
        "​\t一个好的分词器能够帮助模型在面对未知或罕见词汇时保持良好的表现。训练好的分词器能够根据训练语料库中的频率、上下文等信息，对词语进行合理的拆分，让模型在推理和生成时具备更强的泛化能力。例如，遇到罕见或未见过的词时，分词器可以将其分解为多个子词，这样即使模型没有见过完整的单词，也能通过学习到的子词来理解其含义。\n",
        "\n",
        "##### 2.3 **提高模型的效率**\n",
        "\n",
        "​\t分词器还可以显著提高模型的计算效率。较小的词汇表可以减少嵌入矩阵的大小，从而减少模型的参数量和训练时间。此外，通过子词分解，还可以避免对整个单词进行处理，提高模型处理长文本时的效率。\n",
        "\n",
        "#### 3. 分词器的作用\n",
        "\n",
        "##### 3.1 **将文本转换为标记序列**\n",
        "\n",
        "​\t分词器的首要作用是将连续的自然语言文本转换为一系列的标记序列。模型需要处理的是这些离散的标记，而不是原始的字符串。不同的分词策略（如按单词、按子词、按字符等）会影响标记的数量和粒度。选择合适的分词策略对于提高模型的表现非常重要。\n",
        "\n",
        "##### 3.2 **构建词汇表**\n",
        "\n",
        "​\t分词器的另一个作用是基于训练语料库构建一个有限大小的词汇表。这个词汇表包含了模型能够处理的所有标记的集合，分词器在训练时通过分析语料库中的词频和词形变化，选择最适合的子词或词汇构建词汇表，确保词汇表足够小但仍能覆盖大部分文本。\n",
        "\n",
        "##### 3.3 **处理未登录词（OOV问题）**\n",
        "\n",
        "​\t自然语言的词汇是动态且丰富的，在训练数据中可能无法包含所有的词汇。分词器通过将未登录词（Out-Of-Vocabulary，OOV）拆解为子词，能够有效解决OOV问题。例如，对于一个未见过的复杂词汇，分词器可以将其拆分为更常见的子词，从而保证模型可以处理这些新词，而不会因为未登录词的出现导致模型无法理解。\n",
        "\n",
        "##### 3.4 **保持文本的语义一致性**\n",
        "\n",
        "​\t一个好的分词器不仅仅是将文本简单拆解，还需要考虑语义上的一致性。例如，分词器应该尽量避免将具有特定语义的词错误地分割，以免影响模型的语义理解能力。同时，分词器应通过合理的标记组合，使得模型能够在不同的上下文中准确理解词语的含义。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2148b652-7158-4ffb-8500-20f157123111",
      "metadata": {
        "id": "2148b652-7158-4ffb-8500-20f157123111"
      },
      "source": [
        "- 下载requirements.txt并安装相关依赖"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f11a959-a22d-4891-9094-08fa7344dfd5",
      "metadata": {
        "id": "4f11a959-a22d-4891-9094-08fa7344dfd5"
      },
      "source": [
        "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250109200536656.png\" alt=\"image-20250109200536656\" style=\"zoom:33%;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383d05ec-344c-43ea-a1df-abc132e749d3",
      "metadata": {
        "id": "383d05ec-344c-43ea-a1df-abc132e749d3"
      },
      "source": [
        "项目依赖列表如下："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c1772f-1182-40f7-be6c-b18e8d664cc7",
      "metadata": {
        "id": "17c1772f-1182-40f7-be6c-b18e8d664cc7"
      },
      "source": [
        "```bash\n",
        "datasets==2.16.1\n",
        "datasketch==1.6.4\n",
        "Flask==3.0.3\n",
        "Flask_Cors==4.0.0\n",
        "jieba==0.42.1\n",
        "jsonlines==4.0.0\n",
        "marshmallow==3.22.0\n",
        "matplotlib==3.5.1\n",
        "ngrok==1.4.0\n",
        "nltk==3.8\n",
        "numpy==1.26.4\n",
        "openai==1.42.0\n",
        "pandas==1.5.3\n",
        "peft==0.7.1\n",
        "psutil==5.9.8\n",
        "pydantic==2.8.2\n",
        "rich==13.7.1\n",
        "scikit_learn==1.5.1\n",
        "sentence_transformers==2.3.1\n",
        "simhash==2.1.2\n",
        "tiktoken==0.5.1\n",
        "torch==2.1.2\n",
        "transformers==4.44.0\n",
        "jinja2==3.1.2\n",
        "jsonlines==4.0.0\n",
        "trl==0.8.6\n",
        "ujson==5.1.0\n",
        "deepspeed==0.15.1\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3cd2950-f423-4e07-ac8e-e701fde8494c",
      "metadata": {
        "id": "e3cd2950-f423-4e07-ac8e-e701fde8494c"
      },
      "source": [
        "安装项目依赖："
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0dc58ed-fe74-4ddf-b6d9-5a4637b78d40",
      "metadata": {
        "id": "a0dc58ed-fe74-4ddf-b6d9-5a4637b78d40"
      },
      "source": [
        "```bash\n",
        "cd ~/autodl-tmp/miniDeepSeek\n",
        "pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8f3736e-c1fa-42e8-b96c-b84fa7d646da",
      "metadata": {
        "id": "a8f3736e-c1fa-42e8-b96c-b84fa7d646da"
      },
      "source": [
        "- 创建数据集存放文件夹"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e88823c0-af61-44a4-bb56-a672646db3da",
      "metadata": {
        "id": "e88823c0-af61-44a4-bb56-a672646db3da"
      },
      "source": [
        "```bash\n",
        "cd ~/autodl-tmp/miniDeepSeek/\n",
        "mkdir dataset\n",
        "cd ./dataset\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a411d11c-3960-4c72-b593-8ad2ba96944c",
      "metadata": {
        "id": "a411d11c-3960-4c72-b593-8ad2ba96944c"
      },
      "source": [
        "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20241022190406069.png\" alt=\"image-20241022190406069\" style=\"zoom:33%;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "986c029c-690d-43dd-a57d-15f5fc843724",
      "metadata": {
        "id": "986c029c-690d-43dd-a57d-15f5fc843724"
      },
      "source": [
        "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20250109195644948.png\" alt=\"image-20250109195644948\" style=\"zoom:33%;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80f2fbad-9483-4b37-a484-9299e0427369",
      "metadata": {
        "id": "80f2fbad-9483-4b37-a484-9299e0427369"
      },
      "source": [
        "### 二、分词器训练流程"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1504d540-4be5-4e7e-9860-fa2c494dcbda",
      "metadata": {
        "id": "1504d540-4be5-4e7e-9860-fa2c494dcbda"
      },
      "source": [
        "- Step 1.导入必要的库"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b85b0e7-a16c-407c-889b-a15e899bf0d8",
      "metadata": {
        "id": "7b85b0e7-a16c-407c-889b-a15e899bf0d8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29861abc-d6ea-4deb-a437-42601f73cfbc",
      "metadata": {
        "id": "29861abc-d6ea-4deb-a437-42601f73cfbc"
      },
      "source": [
        "- Step 2.读取 tokenizer_train.jsonl 文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88e1dc03-f9be-4ede-b8f3-5422afbf25a5",
      "metadata": {
        "id": "88e1dc03-f9be-4ede-b8f3-5422afbf25a5",
        "outputId": "31594b0a-d4f0-4d76-bd90-2def21e8cdc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "好的。现在请你将这个文本中的所有的逗号都替换成空格。 好的，请稍等一下，现在我会将文本中的所有逗号替换为空格。处理后文本为：\"这是一个句子 目的是看看是否可以正确地从这个句子中删除关键词。\"。处理结果如何？\n",
            "帮我回答一道历史题目。清朝时期的八旗共有多少旗人？ 清朝时期八旗旗人总数约为200万人左右，其中正黄旗、正蓝旗、正白旗、正红旗的人数较多，其他旗的人数较少。\n",
            "嗯，谢谢你介绍的做法很详细，但我不喜欢吃鸡蛋，有没有其他菜做法能介绍一下？ 当然，你可以试试酸辣土豆丝这道菜。\n",
            "材料：\n",
            "土豆2个、红椒1个、青椒1个、大葱1根、醋、生抽、盐、鸡精、料酒\n",
            "做法：\n",
            "1.土豆去皮，切成丝；红椒和青椒切成细丝；大葱切段备用。\n",
            "2.热锅凉油，油热后放入土豆丝，煸炒至变软。\n",
            "3.倒入红椒、青椒和大葱段，继续煸炒至熟。\n",
            "4.加入适量的盐、鸡精、料酒和生抽，翻炒均匀。\n",
            "5.最后，加入适量的醋，翻炒均匀即可。\n",
            "小贴士：\n",
            "1. 土豆切丝时，可以放入淡盐水中泡一下，这样可以去除多余的淀粉。\n",
            "2. 煮土豆丝时，不要煮得太久，以免烂糊。\n",
            "3. 加入醋的时候，根据自己的口味多少来进行调节，一般来说，盐与醋的比例为1:1。\n",
            "4. 如果喜欢辣味可以加入一些干辣椒丝。\n",
            "希望你会喜欢这道酸辣土豆丝！\n",
            "请描述一下如何正确规划个人理财。 正确规划个人理财需要以下几个步骤：\n",
            "1.了解自己的财务状况。这包括收入、支出、资产和负债等信息。了解自己的财务状况可以帮助人们更好地制定财务计划。\n",
            "2.设定财务目标。需要考虑短期目标和长期目标，例如以年为单位设定的支出计划、购房、购车等的长期目标。\n",
            "3.制定预算计划。在了解自己的财务状况并设定财务目标后，需要制定一个预算计划。这可以帮助人们控制支出、节省开支并达到财务目标。\n",
            "4.理性投资和储蓄。人们可以投资于股票、基金、房产或其他投资渠道以实现财务目标。但在投资前需了解相关知识并进行风险评估。同时还应储蓄一定金额，以应对突发事件或为达成某些目标做准备。\n",
            "5.审时度势，合理调整。财务计划需要不断地审时度势，根据实际情况做出调整，以达到最终的财务目标。需要注意财务状况的变化、投资的收益和风险等因素。\n",
            "通过以上五个步骤，人们可以做到合理规划个人理财，掌握自己的财务命运，更好地实现自己的财务目标。\n",
            "描述一下天堂和地狱的生态系统和环境。 天堂和地狱被认为是灵性信仰中关于死后世界的两种不同概念。然而，它们的生态系统和环境都是具有类似特征的极端不同的地方。以下是我对天堂和地狱的生态系统和环境的描述。\n",
            "天堂的生态系统和环境:\n",
            "天堂被描绘为一个美丽、平静、和谐的地方，类似于一片无垢的花园。天堂的生态系统和环境的特征包括:\n",
            "1. 充满和平和爱的氛围。这是一个没有恐惧、痛苦、疾病和死亡的地方。\n",
            "2. 色彩缤纷，充满生机。这是一个绿树成荫、花团锦簇的地方，充满生机和活力。\n",
            "3. 各种生物和动物和谐共存。天使、圣人和各种动物和谐相处，生态系统中没有互相侵害或抢夺资源。\n",
            "4. 充满清新气息的空气。没有污染、烟雾或其他有害物质，空气中充满清新芬芳的气息。\n",
            "5. 物质丰富的环境。天堂中生活着满足需求和愿望的人们，他们拥有一切所需的物质资源，而且没有匮乏、浪费或不公平。\n",
            "地狱的生态系统和环境:\n",
            "地狱被描绘为阴暗、恐怖、嘈杂和可怕的地方。地狱的生态系统和环境的特征包括:\n",
            "1. 充满痛苦和折磨的氛围。这是一个充满恐惧、悔恨和痛苦的地方，全是罪恶的味道。\n",
            "2. 火焰和烈火环绕。地狱中有燃烧的火焰和烈火，许多受罚者被投入火坑中痛苦折磨。\n",
            "3. 恶魔和妖魔横行。地狱中有恶魔、妖怪等可怕的生物，它们在无休止的受苦中享受着自己的又一场比赛。\n",
            "4. 污染和恶臭的气味。地狱中到处都是恶臭和污染，没有清新的气息。\n",
            "5. 没有物质资源。地狱中生活着被惩罚的人们不可能拥有任何物质财富，地狱环境充满了无尽的贫困、饥饿和疾病。\n",
            "综上所述，天堂和地狱是两个完全不同的地方，它们的生态系统和环境反映了它们的性质，体现了人类对不同阶段的死后生命的不同想象和信仰。\n"
          ]
        }
      ],
      "source": [
        "def read_texts_from_jsonl(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            yield data['text']\n",
        "\n",
        "# 测试读取数据\n",
        "data_path = '/root/autodl-tmp/miniDeepSeek/dataset/tokenizer_train.jsonl'\n",
        "texts = read_texts_from_jsonl(data_path)\n",
        "\n",
        "# 打印前几行文本\n",
        "for i, text in enumerate(texts):\n",
        "    if i < 5:\n",
        "        print(text)\n",
        "    else:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "914305ae-436f-47ba-bd1f-6f3b7bbdce61",
      "metadata": {
        "id": "914305ae-436f-47ba-bd1f-6f3b7bbdce61"
      },
      "source": [
        "- Step 3.初始化分词器"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee97e6f5-4081-47de-b741-2a9447851a4e",
      "metadata": {
        "id": "ee97e6f5-4081-47de-b741-2a9447851a4e"
      },
      "source": [
        "首先，通过 `models.BPE()` 创建了一个基于 Byte-Pair Encoding (BPE) 模型的分词器。BPE 是一种常用于文本分词的子词分解算法，特别在自然语言处理任务中被广泛使用，如机器翻译和语言模型训练。BPE 的主要思想是通过将频繁出现的字符或字符对合并成一个新的子词单元，逐步构建一个子词级别的词汇表，从而处理词汇表稀疏性和未登录词问题。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3958bd2a-154c-4d63-ba75-82a36c92b7d4",
      "metadata": {
        "id": "3958bd2a-154c-4d63-ba75-82a36c92b7d4",
        "outputId": "b8af6d7c-efcb-457f-9726-a11a1e3162b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "分词器初始化成功，准备训练。\n"
          ]
        }
      ],
      "source": [
        "# 初始化tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
        "\n",
        "# 定义特殊token\n",
        "special_tokens = [\"<unk>\", \"<s>\", \"</s>\"]\n",
        "\n",
        "# 设置训练器并添加特殊token\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=6400,\n",
        "    special_tokens=special_tokens,  # 确保这三个token被包含\n",
        "    show_progress=True,\n",
        "    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()\n",
        ")\n",
        "\n",
        "print(\"分词器初始化成功，准备训练。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6902f123-7c3e-4d73-be1a-0178ff896f33",
      "metadata": {
        "id": "6902f123-7c3e-4d73-be1a-0178ff896f33"
      },
      "source": [
        "- Step 4.训练分词器"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "346569b9-5ad5-4b77-85af-0a39d9570c14",
      "metadata": {
        "id": "346569b9-5ad5-4b77-85af-0a39d9570c14"
      },
      "source": [
        "【TIME Warning：5mins】"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd14d6e-a14f-47dd-aaa8-7b6e8e60daf5",
      "metadata": {
        "scrolled": true,
        "id": "6bd14d6e-a14f-47dd-aaa8-7b6e8e60daf5",
        "outputId": "a5d679ab-9e78-42ac-db00-970649f093dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "分词器训练完成！\n"
          ]
        }
      ],
      "source": [
        "# 读取文本数据\n",
        "texts = read_texts_from_jsonl(data_path)\n",
        "\n",
        "# 训练tokenizer\n",
        "tokenizer.train_from_iterator(texts, trainer=trainer)\n",
        "\n",
        "print(\"分词器训练完成！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d59390f-092b-46c6-9239-0325a777b234",
      "metadata": {
        "id": "2d59390f-092b-46c6-9239-0325a777b234"
      },
      "source": [
        "- Step 5.保存分词器"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e63eba3-46b7-453b-b871-5d9a54f86704",
      "metadata": {
        "id": "1e63eba3-46b7-453b-b871-5d9a54f86704"
      },
      "source": [
        "在训练完毕之后，还需要设置解码器 (`tokenizer.decoder = decoders.ByteLevel()`) ，这是为了在生成文本时正确地将分词器产生的 token 序列还原回原始文本。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b40976-3741-4b09-9147-08e031e31de7",
      "metadata": {
        "id": "a6b40976-3741-4b09-9147-08e031e31de7",
        "outputId": "d5ce837d-d844-42a5-ba4b-cd4c69ae99f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer 保存成功！\n"
          ]
        }
      ],
      "source": [
        "# 设置解码器\n",
        "tokenizer.decoder = decoders.ByteLevel()\n",
        "\n",
        "# 保存tokenizer\n",
        "tokenizer_dir = \"/root/autodl-tmp/miniDeepSeek/model/miniDeepSeek_tokenizer\"\n",
        "os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "tokenizer.save(os.path.join(tokenizer_dir, \"tokenizer.json\"))\n",
        "tokenizer.model.save(\"/root/autodl-tmp/miniDeepSeek/model/miniDeepSeek_tokenizer\")\n",
        "\n",
        "# 手动创建配置文件\n",
        "config = {\n",
        "    \"add_bos_token\": False,\n",
        "    \"add_eos_token\": False,\n",
        "    \"add_prefix_space\": True,\n",
        "    \"added_tokens_decoder\": {\n",
        "        \"0\": {\n",
        "            \"content\": \"<unk>\",\n",
        "            \"lstrip\": False,\n",
        "            \"normalized\": False,\n",
        "            \"rstrip\": False,\n",
        "            \"single_word\": False,\n",
        "            \"special\": True\n",
        "            },\n",
        "        \"1\": {\n",
        "            \"content\": \"<s>\",\n",
        "            \"lstrip\": False,\n",
        "            \"normalized\": False,\n",
        "            \"rstrip\": False,\n",
        "            \"single_word\": False,\n",
        "            \"special\": True\n",
        "            },\n",
        "        \"2\": {\n",
        "            \"content\": \"</s>\",\n",
        "            \"lstrip\": False,\n",
        "            \"normalized\": False,\n",
        "            \"rstrip\": False,\n",
        "            \"single_word\": False,\n",
        "            \"special\": True\n",
        "            }\n",
        "    },\n",
        "    \"bos_token\": \"<s>\",\n",
        "    \"clean_up_tokenization_spaces\": False,\n",
        "    \"eos_token\": \"</s>\",\n",
        "    \"legacy\": True,\n",
        "    \"model_max_length\": 1000000000000000019884624838656,\n",
        "    \"pad_token\": None,\n",
        "    \"sp_model_kwargs\": {},\n",
        "    \"spaces_between_special_tokens\": False,\n",
        "    \"tokenizer_class\": \"PreTrainedTokenizerFast\",\n",
        "    \"unk_token\": \"<unk>\",\n",
        "    \"use_default_system_prompt\": False,\n",
        "    \"chat_template\": \"{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<s>user\\\\n' + content + '</s>\\\\n<s>assistant\\\\n' }}{% elif message['role'] == 'assistant' %}{{ content + '</s>' + '\\\\n' }}{% endif %}{% endfor %}\"\n",
        "}\n",
        "\n",
        "# 保存配置文件\n",
        "with open(os.path.join(tokenizer_dir, \"tokenizer_config.json\"), \"w\", encoding=\"utf-8\") as config_file:\n",
        "    json.dump(config, config_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(\"Tokenizer 保存成功！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc629abf-ccf4-4957-9c54-df0f65b2fac5",
      "metadata": {
        "id": "dc629abf-ccf4-4957-9c54-df0f65b2fac5"
      },
      "source": [
        "<center><img src=\"https://ml2022.oss-cn-hangzhou.aliyuncs.com/img/image-20241022194241795.png\" alt=\"image-20241022194241795\" style=\"zoom:33%;\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ba1478-98f4-4433-b157-2b5f09b86b91",
      "metadata": {
        "id": "46ba1478-98f4-4433-b157-2b5f09b86b91"
      },
      "source": [
        "- Step 6.评估分词器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "555b238b-d349-4f03-a33c-50edd757eb5c",
      "metadata": {
        "id": "555b238b-d349-4f03-a33c-50edd757eb5c",
        "outputId": "cc5adc6b-bd1e-40a8-e446-04a2e0456ef3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[608, 1589, 4835, 269, 4833, 954, 4725, 270, 1170, 345, 4584, 5204, 1273, 648, 2207, 1, 320, 275, 201, 345, 1390, 258, 3852, 1081, 269, 2, 201, 1, 1078, 538, 501, 201, 22, 23, 24, 2, 201, 1, 320, 275, 201, 22, 23, 24, 2, 201, 1, 1078, 538, 501, 201, 25, 26, 27, 2, 201]\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 加载预训练的tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./model/miniDeepSeek_tokenizer\")\n",
        "\n",
        "# 测试一段对话\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是一个优秀的聊天机器人，总是给我正确的回应！\"},\n",
        "    {\"role\": \"user\", \"content\": '是椭圆形的'},\n",
        "    {\"role\": \"assistant\", \"content\": '456'},\n",
        "    {\"role\": \"user\", \"content\": '456'},\n",
        "    {\"role\": \"assistant\", \"content\": '789'}\n",
        "]\n",
        "\n",
        "# 使用模板进行文本处理\n",
        "new_prompt = tokenizer.apply_chat_template(messages, tokenize=True)\n",
        "print(new_prompt)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}